{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gobot_md_yaml_configs_tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepmipt/DeepPavlov/blob/master/examples/gobot_md_yaml_configs_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj5nbxnPbz6D",
        "colab_type": "text"
      },
      "source": [
        "# How Do I: Build Go-Bot Skill with RASA DSLs (v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRO97JLXb8j5",
        "colab_type": "text"
      },
      "source": [
        "At DeepPavlov, we support a variety of industry-wide and popular standards to support developing Conversational AI solutions.\n",
        "DSLs, known as Domain-Specific Languages, provide a rich mechanism to define the behavior, or \"the what\", while\n",
        "the underlying system uses the parser to transform these definitions into commands that implement this behavior, or \"the how\" using the system's components.\n",
        "\n",
        "In this tutorial, you will learn how to use another industrial DSL, or, better said, set of DSLs, introduced by RASA.ai,\n",
        "to build simple goal-oriented chatbots using DeepPavlov's GO-bot.\n",
        "\n",
        "This is the very beginning of our work focused on supporting RASA DSLs as a way to configure DeepPavlov-based goal-oriented chatbots,\n",
        "and therefore not all elements of the RASA DSLs are supported. It is also worth mentioning that in 0.12.0 release we want to focus on supporting tools \n",
        "to define the domain logic behind the goal-oriented assistant, and files like `config.yml` and others are out of scope for this release.\n",
        "\n",
        "To configure a DeepPavlov-based goal-oriented chatbot using these DSLs, you need to have at least three basic config files:\n",
        "* `stories.md` (or `stories-{trn, tst, val}.md` but these are just subsamples)\n",
        "* `nlu.md`\n",
        "* `domain.yml`\n",
        "\n",
        "These files allow you to define 3 key elements of the chatbot, including product-level stories, NLU training data, and your chatbot's domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWP08HJGnCDo",
        "colab_type": "text"
      },
      "source": [
        "## Concepts Behind Stories.md, NLU.md, and Domain.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_NQHZOqcwcr",
        "colab_type": "text"
      },
      "source": [
        "### `stories.md`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3HWMZPrg4V1",
        "colab_type": "text"
      },
      "source": [
        "`stories.md` is a mechanism used to teach your chatbot how to respond to user messages. It allows you to control your chatbot's dialog management.\n",
        "\n",
        "These \"stories\" model real conversations between a user and a chatbot. This Markdown-based file is used to define a list of\n",
        "*stories*, and each *story* can have a list of one or more *intents* with (optional) corresponding *slots*, where each *intent*\n",
        "has one or more corresponding *actions* taken by the chatbot.\n",
        "\n",
        "These actions, in general, can be anything, from simple message replies, to programmable actions that call APIs of other services.\n",
        "*Note:* In this version, supported actions are limited to simple message replies.\n",
        "\n",
        "In a way, it can be seen as a *dialogues dataset*.\n",
        "\n",
        "*Note: Stories do not provide an ultimative instruction of how the bot should behave: it is up to the training process to infer the implicit underlying patterns controlling the dialogue.*  \n",
        "\n",
        "If you are looking for a way to make the bot follow the story templates strictly as defined, \n",
        "there is a known hack: \n",
        "the more times the model sees the training data, the better the model models the data, \n",
        "so the the desired behavior is achieved when the accuracy on the training data is 1.\n",
        "Such a situation is illustrated in section Basic Chatbot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0oZHwh4K6PG",
        "colab_type": "text"
      },
      "source": [
        "#### format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad2ekquvK8oo",
        "colab_type": "text"
      },
      "source": [
        "Stories file is a markdown file of the following format:\n",
        "\n",
        "```markdown\n",
        "## story_title(not used by algorithm, but useful to work with for humans)\n",
        "* user_action_label{\"1st_slot_present_in_action\": \"slot1_value\", .., \"Nth_slot_present_in_action\": \"slotN_value\"}\n",
        " - system_respective_utterance\n",
        "* another_user_action_of_the_same_format\n",
        "  - another_system_response\n",
        "...\n",
        "\n",
        "## another_story_title\n",
        "...\n",
        "\n",
        "```\n",
        "\n",
        "**See examples below in this tutorial**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhqxjj_Pedlo",
        "colab_type": "text"
      },
      "source": [
        "### `nlu.md`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zK1TuEvg6KJ",
        "colab_type": "text"
      },
      "source": [
        "`nlu.md` represents an NLU model of your chatbot. It allows you to provide training examples that show how your chatbot should\n",
        "understand user messages, and then train a model through these examples.\n",
        "\n",
        "While DeepPavlov's GO-bot supports JSON-based DSTC-2 format for training data, this Markdown Format introduced by RASA is the easiest one for humans to read and write."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVR4J_UNMAgq",
        "colab_type": "text"
      },
      "source": [
        "#### format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEj0G5ciMBtA",
        "colab_type": "text"
      },
      "source": [
        "NLU file is a markdown file of the following format:\n",
        "\n",
        "```markdown\n",
        "## intent:possible_user_action_label_1\n",
        "- An example of user text that has the possible_user_action_label_1 action label\n",
        "- Another example of user text that has the possible_user_action_label_1 action label\n",
        "...\n",
        "\n",
        "## intent:possible_user_action_label_N\n",
        "- An example of user text that has the (possible_user_action_label_N)[action_label] action label\n",
        "<!-- Slotfilling dataset is provided as an inline markup of user texts -->\n",
        "...\n",
        "\n",
        "```\n",
        "\n",
        "**See examples below in this tutorial**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-wQb3XMfqVJ",
        "colab_type": "text"
      },
      "source": [
        "### `domain.yml`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIlJd3FIg8Hg",
        "colab_type": "text"
      },
      "source": [
        "`domain.yml` helps you to define the universe your chatbot lives in: what user inputs it expects to get, what actions it should be able to predict,\n",
        "how to respond, and what information to store.\n",
        "This YML format is relatively simple, and it can be seen as a dictionary of all components of your chatbot, including but not limited to intents,\n",
        "actions, responses, and other things."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5qoE7NnNTEg",
        "colab_type": "text"
      },
      "source": [
        "#### format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl5QCyVMOpU9",
        "colab_type": "text"
      },
      "source": [
        "Domain file is a YAML file of the following format:\n",
        "\n",
        "```yaml\n",
        "# slots section lists the possible slot names (aka slot types) \n",
        "# that are used in the domain (i.e. relevant for bot's tasks)\n",
        "# currently only type: text is supported\n",
        "slots:\n",
        "  slot1_name:\n",
        "    type: text\n",
        "  ...\n",
        "  slotN_name:\n",
        "    type: text\n",
        "\n",
        "# entities list now follows the slots list 2nd level keys \n",
        "# and is present to support upcoming features. Stay tuned for updates with this!\n",
        "entities:\n",
        "- slot1_name\n",
        "...\n",
        "- slotN_name\n",
        "\n",
        "# intents section lists the intents that can appear in the stories\n",
        "# being kept together they do describe the user-side part of go-bot's experience\n",
        "intents:\n",
        "  - user_action_label\n",
        "  - another_user_action_of_the_same_format\n",
        "  ...\n",
        "\n",
        "# responses section lists the system response templates.\n",
        "# Despite system response' titles being usually informative themselves\n",
        "#   (one could even find them more appropriate when no actual \"Natural Language\" is needed \n",
        "#    (e.g. for buttons actions in bot apps))\n",
        "# It is though extremely useful to be able to serialize the response title to text. \n",
        "# That's what this section content is needed for.\n",
        "responses:\n",
        "  system_utterance_1:\n",
        "    - text: \"The text that system responds with\"\n",
        "  another_system_response:\n",
        "    - text: \"Here some text again\"\n",
        "\n",
        "```\n",
        "\n",
        "**See examples below in this tutorial**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G__SfMVanNGc",
        "colab_type": "text"
      },
      "source": [
        "## Basic Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAFtnDz4Wbyg",
        "colab_type": "text"
      },
      "source": [
        "Let's build the simplest chatbot possible.\n",
        "This chatbot will be capable of processing three intents: *greeting*, *goodbye*, and *thanks*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BElx8chGte5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DP_MIN_DEMO_DIR = \"dp_minimal_demo_dir\"  # we will work in this folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ypZ3rr1ta44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dee6d78c-35ef-43be-9176-341c033fd88b"
      },
      "source": [
        "import os\n",
        "\n",
        "%cd /content\n",
        "os.makedirs(DP_MIN_DEMO_DIR, exist_ok=True)\n",
        "%cd {DP_MIN_DEMO_DIR}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/dp_minimal_demo_dir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6lryyfyzPio",
        "colab_type": "text"
      },
      "source": [
        "### Stories.md: Basic Stories Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwJPzxgqWrgf",
        "colab_type": "text"
      },
      "source": [
        "`stories.md` is pretty straightforward in this case. In it you define 3 stories, each having its own intent and response (utterance).\n",
        "Take into account the fact that you can combine all of these intents under one story, or add two intents to one story, and third to another one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsVgqFHxnUFq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c3bff00-58f1-4bec-cf8f-3a8f01f8e69b"
      },
      "source": [
        "%%writefile stories.md\n",
        "\n",
        "## greet\n",
        "* greet\n",
        "  - utter_greet\n",
        "\n",
        "## thank\n",
        "* thank\n",
        "  - utter_noworries\n",
        "\n",
        "## goodbye\n",
        "* bye\n",
        "  - utter_bye"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing stories.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYDaQMulzPiy",
        "colab_type": "text"
      },
      "source": [
        "### nlu.md: Basic NLU Training Data Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz_qOBjaW2v7",
        "colab_type": "text"
      },
      "source": [
        "`nlu.md` has an NLU training data that enables DeepPavlov to recognize user phrases as belonging to one of the intents defined in `domain.yml`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk1GzeqAuK59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eb9e3c3f-9e13-4689-93e4-5a45025ec4e7"
      },
      "source": [
        "%%writefile nlu.md\n",
        "\n",
        "## intent:greet\n",
        "- Hi\n",
        "- Hey\n",
        "- Hi bot\n",
        "- Hey bot\n",
        "- Hello\n",
        "- Good morning\n",
        "- hi again\n",
        "- hi folks\n",
        "\n",
        "## intent:bye\n",
        "- goodbye\n",
        "- goodnight\n",
        "- good bye\n",
        "- good night\n",
        "- see ya\n",
        "- toodle-oo\n",
        "- bye bye\n",
        "- gotta go\n",
        "- farewell\n",
        "\n",
        "## intent:thank\n",
        "- Thanks\n",
        "- Thank you\n",
        "- Thank you so much\n",
        "- Thanks bot\n",
        "- Thanks for that\n",
        "- cheers\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing nlu.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gkE0GuRzPjA",
        "colab_type": "text"
      },
      "source": [
        "### domain.yml: Basic Domain Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt8b1LWTW8mM",
        "colab_type": "text"
      },
      "source": [
        "In this demo, `domain.yml` contains the list of: \n",
        "* possible user action intents, and\n",
        "* possible system response actions\n",
        "\n",
        "*Note:* Entities and slots are omitted in this example. See the more sophisticated example below to see how they can be defined in the `domain.yml`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srHE4tBhvMW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c34ddd0-207c-4988-8d03-8a95ec215b4f"
      },
      "source": [
        "%%writefile domain.yml\n",
        "\n",
        "intents:\n",
        "  - greet\n",
        "  - bye\n",
        "  - thank\n",
        "\n",
        "responses:\n",
        "  utter_noworries:\n",
        "  - text: No worries!\n",
        "  utter_greet:\n",
        "  - text: Hi\n",
        "  utter_bye:\n",
        "  - text: Bye!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing domain.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75BDcU4yXY1g",
        "colab_type": "text"
      },
      "source": [
        "The next step is to install the `deeppavlov` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcp4-WpCzHaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02326789-f100-4510-b2bf-5722d901f1da"
      },
      "source": [
        "!pip install git+https://github.com/deepmipt/DeepPavlov.git@feature/gobot-md-yaml-config\n",
        "!python -m deeppavlov install gobot_simple_dstc2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/deepmipt/DeepPavlov.git@feature/gobot-md-yaml-config\n",
            "  Cloning https://github.com/deepmipt/DeepPavlov.git (to revision feature/gobot-md-yaml-config) to /tmp/pip-req-build-dsjafm4a\n",
            "  Running command git clone -q https://github.com/deepmipt/DeepPavlov.git /tmp/pip-req-build-dsjafm4a\n",
            "  Running command git checkout -b feature/gobot-md-yaml-config --track origin/feature/gobot-md-yaml-config\n",
            "  Switched to a new branch 'feature/gobot-md-yaml-config'\n",
            "  Branch 'feature/gobot-md-yaml-config' set up to track remote branch 'feature/gobot-md-yaml-config' from 'origin'.\n",
            "Collecting aio-pika==6.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/07/196a4115cbef31fa0c3dabdea146f02dffe5e49998341d20dbe2278953bc/aio_pika-6.4.1-py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.1MB/s \n",
            "\u001b[?25hCollecting Cython==0.29.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/d1/4d3f8a7a920e805488a966cc6ab55c978a712240f584445d703c08b9f405/Cython-0.29.14-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 6.4MB/s \n",
            "\u001b[?25hCollecting fastapi==0.47.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/a7/4804d7abf8a1544d079d50650af872387154ebdac5bd07d54b2e60e2b334/fastapi-0.47.1-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov==0.11.0) (2.10.0)\n",
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 18.5MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/45f71bd24f4e37629e9db5fb75caab919507deae6a5a257f9e4685a5f931/numpy-1.18.0-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.4MB/s \n",
            "\u001b[?25hCollecting overrides==2.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n",
            "Collecting pandas==0.25.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 45.8MB/s \n",
            "\u001b[?25hCollecting pytz==2019.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 46.6MB/s \n",
            "\u001b[?25hCollecting pydantic==1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/24/e78cf017628e7eaed20cb040999b1ecc69f872da53dfd0d9aed40c0fa5f1/pydantic-1.3-cp36-cp36m-manylinux2010_x86_64.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 21.6MB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0MB 18.8MB/s \n",
            "\u001b[?25hCollecting pyopenssl==19.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
            "\u001b[?25hCollecting pytelegrambotapi==3.6.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/99c606f69fcda57e35788b913dd34c9d9acb48dd26349141b3855dcf6351/pyTelegramBotAPI-3.6.7.tar.gz (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.2MB/s \n",
            "\u001b[?25hCollecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml==0.15.100\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/9f/83bb34eaf84032b0b54fcc4a6aff1858572d279d65a301c7ae875f523df5/ruamel.yaml-0.15.100-cp36-cp36m-manylinux1_x86_64.whl (656kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 21.6MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n",
            "Collecting scikit-learn==0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/04/49633f490f726da6e454fddc8e938bbb5bfed2001681118d3814c219b723/scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov==0.11.0) (1.4.1)\n",
            "Requirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov==0.11.0) (4.41.1)\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.6/dist-packages (from deeppavlov==0.11.0) (7.1.2)\n",
            "Collecting uvicorn==0.11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/11f4b4bf3963ead6de570feeae49eeced02f6768cf1f68e16f4b16d3b0aa/uvicorn-0.11.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 45.1MB/s \n",
            "\u001b[?25hCollecting yarl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/b4/2cbeaf2c3ea53865d9613b315fe24e78c66acedb1df7e4be4e064c87203b/yarl-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (257kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 35.8MB/s \n",
            "\u001b[?25hCollecting aiormq<4,>=3.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/c6/4a9f8f22eef268289e9af5da6a620d837c700b333eae01132bfe48fe7dc9/aiormq-3.2.3-py3-none-any.whl\n",
            "Collecting starlette<=0.12.9,>=0.12.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/2220fe5bf287e693a6430d8ee36c681b0157035b7249ec08f8fb36319d16/starlette-0.12.9.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py==2.10.0->deeppavlov==0.11.0) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->deeppavlov==0.11.0) (2.8.1)\n",
            "Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic==1.3->deeppavlov==0.11.0) (0.7)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov==0.11.0) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 17.2MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/91/84a29d6a27fd6dfc21f475704c4d2053d58ed7a4033c2b0ce1b4ca4d03d9/cryptography-3.0-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov==0.11.0) (3.0.4)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov==0.11.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov==0.11.0) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.2->deeppavlov==0.11.0) (0.16.0)\n",
            "Collecting httptools==0.0.13; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/03/215969db11abe8741e9c266a4cbe803a372bd86dd35fa0084c4df6d4bd00/httptools-0.0.13.tar.gz (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 45.9MB/s \n",
            "\u001b[?25hCollecting uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/48/586225bbb02d3bdca475b17e4be5ce5b3f09da2d6979f359916c1592a687/uvloop-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 41.8MB/s \n",
            "\u001b[?25hCollecting websockets==8.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.1MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n",
            "\u001b[?25hCollecting multidict>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/95/f50352b5366e7d579e8b99631680a9e32e1b22adfa1629a8f23b1d22d5e2/multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov==0.11.0) (3.7.4.2)\n",
            "Collecting pamqp==2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/56/afa06143361e640c9159d828dadc95fc9195c52c95b4a97d136617b0166d/pamqp-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov==0.11.0) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov==0.11.0) (2.20)\n",
            "Building wheels for collected packages: deeppavlov, nltk, overrides, pytelegrambotapi, sacremoses, starlette, httptools\n",
            "  Building wheel for deeppavlov (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeppavlov: filename=deeppavlov-0.11.0-cp36-none-any.whl size=868901 sha256=8d381700133ba65488065f3d11170018e546b544f99381822a018fb003cace4d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xj3rmgnb/wheels/d1/14/83/51a15d3d0cc9bf87e735c1daddf375ad226313aa14d8cd04d9\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449904 sha256=abd33de602e666a0519f7eeb5aadb88cd4981b55c72215a8193d28298b873841\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-cp36-none-any.whl size=5600 sha256=ed0b5e549f25d268b04ff9a823ded092cc3825c623c1effe34c06b5ef1af450f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-cp36-none-any.whl size=47178 sha256=6155ced5c9782df9e73fc6d60477302816d044ffd0b18bb7c1aea50c1227694f\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/40/18/8a34153f95ef0dc19e3954898e5a5079244b76a8afdd7d0ec5\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=2f77eba200f0f208307aa357b3c447c6101cb1cf5a233a2ebb7ce8dd17107083\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for starlette: filename=starlette-0.12.9-cp36-none-any.whl size=57245 sha256=0ec4f632b60d26bb2c34ab359f9e0d6f487ede569ee69875b2f7cc0e4eca453a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5\n",
            "  Building wheel for httptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for httptools: filename=httptools-0.0.13-cp36-cp36m-linux_x86_64.whl size=212529 sha256=14507ee4f456aac42aabbef17af9bc0bcd367f2712fdf2f7ce84d04c3dcd0941\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/3e/2e/013f99b42efc25cf3589730cf380738e46b1e5edaf2f78d525\n",
            "Successfully built deeppavlov nltk overrides pytelegrambotapi sacremoses starlette httptools\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: multidict, idna, yarl, pamqp, aiormq, aio-pika, Cython, pydantic, starlette, fastapi, nltk, numpy, overrides, pytz, pandas, dawg-python, pymorphy2-dicts, pymorphy2, pymorphy2-dicts-ru, cryptography, pyopenssl, requests, pytelegrambotapi, ruamel.yaml, rusenttokenize, scikit-learn, httptools, uvloop, websockets, h11, uvicorn, sacremoses, deeppavlov\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: Cython 0.29.21\n",
            "    Uninstalling Cython-0.29.21:\n",
            "      Successfully uninstalled Cython-0.29.21\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: pandas 1.0.5\n",
            "    Uninstalling pandas-1.0.5:\n",
            "      Successfully uninstalled pandas-1.0.5\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.2.3 cryptography-3.0 dawg-python-0.7.2 deeppavlov-0.11.0 fastapi-0.47.1 h11-0.9.0 httptools-0.0.13 idna-2.8 multidict-4.7.6 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.404381.4453942 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 starlette-0.12.9 uvicorn-0.11.1 uvloop-0.14.0 websockets-8.1 yarl-1.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas",
                  "pytz"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:16:57.899 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'gobot_simple_dstc2' as '/usr/local/lib/python3.6/dist-packages/deeppavlov/configs/go_bot/gobot_simple_dstc2.json'\n",
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 61.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (1.18.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (1.15.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.1) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.1) (1.14.33)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.1) (2.22.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.1) (1.17.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.1) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.1) (0.10.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (2020.6.20)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (2.8)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->smart-open>=1.8.1->gensim==3.8.1) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->smart-open>=1.8.1->gensim==3.8.1) (2.8.1)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.1\n",
            "Collecting spacy==2.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/13/80ad28ef7a16e2a86d16d73e28588be5f1085afd3e85e4b9b912bd700e8a/spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (0.7.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (1.18.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (2.22.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (49.2.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (1.0.2)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/59/6bb553bc9a5f072d3cd479fc939fea0f6f682892f1f5cff98de5c9b615bb/thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 34.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (3.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2020.6.20)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (1.24.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.2.3) (3.1.0)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed spacy-2.2.3 thinc-7.3.1\n",
            "Collecting rapidfuzz==0.7.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/4e/66d1ad1ad9f7f1e82c68632a6683c2408cc0e134cebf1a67572370379557/rapidfuzz-0.7.6-cp36-cp36m-manylinux2010_x86_64.whl (689kB)\n",
            "\u001b[K     |████████████████████████████████| 696kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-0.7.6\n",
            "Collecting tensorflow==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d9/fd234c7bf68638423fb8e7f44af7fcfce3bcaf416b51e6d902391e47ec43/tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 97kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 40.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.30.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.18.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.9.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.12.4)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (49.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=332fd9d3e350b2e52c8d33259f18427553f27a5ffe9a803eca6967639a8224a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, keras-applications, tensorboard, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.2.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGxtOE-fXiUN",
        "colab_type": "text"
      },
      "source": [
        "Define the path to our DSL-based configuration files above (the folder we are in right now) and the folder used to store the trained bot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcC7aDfk6iiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from deeppavlov import configs\n",
        "from deeppavlov.core.common.file import read_json\n",
        "\n",
        "\n",
        "gobot_config = read_json(configs.go_bot.gobot_md_yaml_minimal)\n",
        "\n",
        "gobot_config['metadata']['variables']['DATA_PATH'] = '.'\n",
        "gobot_config['metadata']['variables']['MODEL_PATH'] = '.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOenbeUvXwW-",
        "colab_type": "text"
      },
      "source": [
        "Since our data is basically the mock tutorial data we will use the same subsamples for all of the train (training set), test (test set) and valid (validation set) subsamples. \n",
        "\n",
        "However, for a real DeepPavlov-based goal-oriented bot you should use different train, test, and valid sample stories.md files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reCVX2Mz7W8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp stories.md stories-trn.md\n",
        "!cp stories.md stories-tst.md \n",
        "!cp stories.md stories-val.md "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjPL3dqzYj3B",
        "colab_type": "text"
      },
      "source": [
        "The next step is to train the bot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS6hIi9m7Sev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46f89511-4a0b-4b1a-afe6-8a961cb5e12b"
      },
      "source": [
        "from deeppavlov import train_model\n",
        "\n",
        "train_model(gobot_config, download=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:18:09.343 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt to /root/.deeppavlov/downloads/embeddings/glove.6B.100d.txt\n",
            "347MB [01:35, 3.65MB/s]\n",
            "2020-08-07 08:19:47.73 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/datasets/gobot_md_yaml_minimal.tar.gz to /content/gobot_md_yaml_minimal.tar.gz\n",
            "100%|██████████| 528/528 [00:00<00:00, 1.53MB/s]\n",
            "2020-08-07 08:19:47.749 INFO in 'deeppavlov.core.data.utils'['utils'] at line 269: Extracting /content/gobot_md_yaml_minimal.tar.gz archive into /content/dp_minimal_demo_dir\n",
            "2020-08-07 08:19:47.769 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpixkz464w]\n",
            "2020-08-07 08:19:47.776 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpfcnmbf3s]\n",
            "2020-08-07 08:19:47.784 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpav_5p6vz]\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "2020-08-07 08:19:50.529 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/dp_minimal_demo_dir/word.dict]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:19:52.683 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpaw6xcn4n]\n",
            "2020-08-07 08:19:52.688 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpnug3ke4x]\n",
            "2020-08-07 08:19:52.693 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpsrzgoi32]\n",
            "2020-08-07 08:19:52.926 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `/root/.deeppavlov/downloads/embeddings/glove.6B.100d.txt`]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/common/registry.py:40: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:194: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/go_bot/policy/policy_network.py:250: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/go_bot/policy/policy_network.py:265: The name tf.nn.rnn_cell.LSTMStateTuple is deprecated. Please use tf.compat.v1.nn.rnn_cell.LSTMStateTuple instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/go_bot/policy/policy_network.py:276: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/go_bot/policy/policy_network.py:285: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/go_bot/policy/policy_network.py:292: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/go_bot/policy/policy_network.py:246: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:122: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:127: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:127: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py:2825: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/go_bot/policy/policy_network.py:79: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/go_bot/policy/policy_network.py:335: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:36.880 INFO in 'deeppavlov.models.go_bot.policy.policy_network'['policy_network'] at line 89: INSIDE PolicyNetwork init(). Initializing PolicyNetwork from scratch.\n",
            "2020-08-07 08:20:36.949 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 198: Initial best per_item_action_accuracy of 0.8333\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 0.8333}, \"time_spent\": \"0:00:01\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:37.301 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best per_item_action_accuracy of 1.0\n",
            "2020-08-07 08:20:37.302 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
            "2020-08-07 08:20:37.308 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/dp_minimal_demo_dir/model/policy]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"train\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:01\", \"epochs_done\": 14, \"batches_seen\": 15, \"train_examples_seen\": 45, \"learning_rate\": 0.003, \"momentum\": 0.95, \"loss\": 0.5221795449654262}}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:77: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:01\", \"epochs_done\": 14, \"batches_seen\": 15, \"train_examples_seen\": 45, \"impatience\": 0, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:37.557 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the per_item_action_accuracy of 1.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"train\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:01\", \"epochs_done\": 29, \"batches_seen\": 30, \"train_examples_seen\": 90, \"learning_rate\": 0.003, \"momentum\": 0.95, \"loss\": 0.02861070235570272}}\n",
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:01\", \"epochs_done\": 29, \"batches_seen\": 30, \"train_examples_seen\": 90, \"impatience\": 1, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:37.695 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the per_item_action_accuracy of 1.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"train\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:01\", \"epochs_done\": 44, \"batches_seen\": 45, \"train_examples_seen\": 135, \"learning_rate\": 0.003, \"momentum\": 0.95, \"loss\": 0.023759933809439342}}\n",
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:01\", \"epochs_done\": 44, \"batches_seen\": 45, \"train_examples_seen\": 135, \"impatience\": 2, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:37.848 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the per_item_action_accuracy of 1.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"train\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:01\", \"epochs_done\": 59, \"batches_seen\": 60, \"train_examples_seen\": 180, \"learning_rate\": 0.003, \"momentum\": 0.95, \"loss\": 0.022868612532814345}}\n",
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:01\", \"epochs_done\": 59, \"batches_seen\": 60, \"train_examples_seen\": 180, \"impatience\": 3, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:38.5 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the per_item_action_accuracy of 1.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"train\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 74, \"batches_seen\": 75, \"train_examples_seen\": 225, \"learning_rate\": 0.003, \"momentum\": 0.95, \"loss\": 0.021253203103939692}}\n",
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 74, \"batches_seen\": 75, \"train_examples_seen\": 225, \"impatience\": 4, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:38.157 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the per_item_action_accuracy of 1.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"train\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 89, \"batches_seen\": 90, \"train_examples_seen\": 270, \"learning_rate\": 0.003, \"momentum\": 0.95, \"loss\": 0.019334316005309424}}\n",
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 89, \"batches_seen\": 90, \"train_examples_seen\": 270, \"impatience\": 5, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:38.318 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the per_item_action_accuracy of 1.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"train\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 104, \"batches_seen\": 105, \"train_examples_seen\": 315, \"learning_rate\": 0.003, \"momentum\": 0.95, \"loss\": 0.017390433450539908}}\n",
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 104, \"batches_seen\": 105, \"train_examples_seen\": 315, \"impatience\": 6, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:38.463 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the per_item_action_accuracy of 1.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"train\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 119, \"batches_seen\": 120, \"train_examples_seen\": 360, \"learning_rate\": 0.003, \"momentum\": 0.95, \"loss\": 0.015567492072780928}}\n",
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 119, \"batches_seen\": 120, \"train_examples_seen\": 360, \"impatience\": 7, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:38.603 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the per_item_action_accuracy of 1.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"train\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 134, \"batches_seen\": 135, \"train_examples_seen\": 405, \"learning_rate\": 0.003, \"momentum\": 0.95, \"loss\": 0.013920772324005763}}\n",
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 134, \"batches_seen\": 135, \"train_examples_seen\": 405, \"impatience\": 8, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:38.736 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the per_item_action_accuracy of 1.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"train\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 149, \"batches_seen\": 150, \"train_examples_seen\": 450, \"learning_rate\": 0.003, \"momentum\": 0.95, \"loss\": 0.012460200923184554}}\n",
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 149, \"batches_seen\": 150, \"train_examples_seen\": 450, \"impatience\": 9, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:38.874 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the per_item_action_accuracy of 1.0\n",
            "2020-08-07 08:20:38.875 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 307: Ran out of patience\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"train\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 164, \"batches_seen\": 165, \"train_examples_seen\": 495, \"learning_rate\": 0.003, \"momentum\": 0.95, \"loss\": 0.011176881504555543}}\n",
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 164, \"batches_seen\": 165, \"train_examples_seen\": 495, \"impatience\": 10, \"patience_limit\": 10}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:20:38.957 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/dp_minimal_demo_dir/word.dict]\n",
            "2020-08-07 08:20:38.963 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpc776jzt1]\n",
            "2020-08-07 08:20:38.971 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpbxxft_p5]\n",
            "2020-08-07 08:20:38.976 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmp96uk16qt]\n",
            "2020-08-07 08:20:38.980 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `/root/.deeppavlov/downloads/embeddings/glove.6B.100d.txt`]\n",
            "2020-08-07 08:21:21.299 INFO in 'deeppavlov.models.go_bot.policy.policy_network'['policy_network'] at line 86: INSIDE PolicyNetwork init(). Initializing PolicyNetwork from checkpoint.\n",
            "2020-08-07 08:21:21.305 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/dp_minimal_demo_dir/model/policy]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/dp_minimal_demo_dir/model/policy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:21:21.497 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/dp_minimal_demo_dir/word.dict]\n",
            "2020-08-07 08:21:21.504 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpuh5367dn]\n",
            "2020-08-07 08:21:21.509 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpywctqej1]\n",
            "2020-08-07 08:21:21.513 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmplvww1r9m]\n",
            "2020-08-07 08:21:21.516 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `/root/.deeppavlov/downloads/embeddings/glove.6B.100d.txt`]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:01\"}}\n",
            "{\"test\": {\"eval_examples_count\": 3, \"metrics\": {\"per_item_action_accuracy\": 1.0}, \"time_spent\": \"0:00:01\"}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:22:03.649 INFO in 'deeppavlov.models.go_bot.policy.policy_network'['policy_network'] at line 86: INSIDE PolicyNetwork init(). Initializing PolicyNetwork from checkpoint.\n",
            "2020-08-07 08:22:03.655 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/dp_minimal_demo_dir/model/policy]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/dp_minimal_demo_dir/model/policy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Chainer[<deeppavlov.models.go_bot.wrapper.DialogComponentWrapper at 0x7f4ec0098ef0>,\n",
              "        <deeppavlov.models.go_bot.go_bot.GoalOrientedBot at 0x7f4eb8a85a58>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tenEY1QVYmlg",
        "colab_type": "text"
      },
      "source": [
        "Finally, it's time to build our bot and experiment with it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmK7I_06xXcH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "0a96a22a-8cf2-4761-f53f-7312fdbd503d"
      },
      "source": [
        "from deeppavlov import build_model\n",
        "bot = build_model(gobot_config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:22:03.723 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/dp_minimal_demo_dir/word.dict]\n",
            "2020-08-07 08:22:03.730 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmp2efvnljv]\n",
            "2020-08-07 08:22:03.734 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpzce_6p7a]\n",
            "2020-08-07 08:22:03.739 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpxdsb4m9t]\n",
            "2020-08-07 08:22:03.743 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `/root/.deeppavlov/downloads/embeddings/glove.6B.100d.txt`]\n",
            "2020-08-07 08:22:45.921 INFO in 'deeppavlov.models.go_bot.policy.policy_network'['policy_network'] at line 86: INSIDE PolicyNetwork init(). Initializing PolicyNetwork from checkpoint.\n",
            "2020-08-07 08:22:45.926 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/dp_minimal_demo_dir/model/policy]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/dp_minimal_demo_dir/model/policy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2hpK1G0x0gu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5d908114-30be-43a8-e1d8-64330d751f46"
      },
      "source": [
        "bot.reset()\n",
        "\n",
        "bot([\"start\"])\n",
        "bot([\"Hi\"])[0][0].actions_tuple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('utter_greet',)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbUm-l73Yvry",
        "colab_type": "text"
      },
      "source": [
        "Our bot answers with \"greeting\" to our \"greeting\". How will it respond to some grateful message?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKQ86ESXx9GV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3741ddfd-f508-45b4-cd11-8f4ed59ef417"
      },
      "source": [
        "bot.reset()\n",
        "bot([\"start\"])\n",
        "\n",
        "bot([\"Thanks!\"])[0][0].actions_tuple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('utter_noworries',)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvkEelaEY3kb",
        "colab_type": "text"
      },
      "source": [
        "Ok, \"no worries\" is an expected response. Let's check if the \"goodbye\" user message is processed with the corresponding reply:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM9xCLMs5Ke5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bot.reset()\n",
        "bot([\"start\"])\n",
        "\n",
        "bot_response_actions = bot([\"bye\"])[0][0].actions_tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhqKmwbO6K1v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0d296cf1-9652-4002-b863-06c1c4b43807"
      },
      "source": [
        "import yaml\n",
        "\n",
        "system_utter2text = yaml.load(open(\"domain.yml\"))[\"responses\"]\n",
        "system_utter2text[bot_response_actions[0]][0][\"text\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bye!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJUFlCjinRsP",
        "colab_type": "text"
      },
      "source": [
        "## Advanced Chatbot: Building a Restaurants Bot inspired by the DSTC Schema-Guided Dialogue Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilk5KxbBi98e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DP_BIG_DEMO_DIR = \"dp_big_demo_dir\"  # we'll work in this directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqCEXGyEjDrQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e4fc9015-0f99-412d-be87-5a230061ad57"
      },
      "source": [
        "import os\n",
        "\n",
        "%cd /content\n",
        "os.makedirs(DP_BIG_DEMO_DIR, exist_ok=True)\n",
        "%cd {DP_BIG_DEMO_DIR}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/dp_big_demo_dir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ4q33U39pMp",
        "colab_type": "text"
      },
      "source": [
        "While the previous demo was focused on figuring out how to work with the very simple goal-oriented chatbot, the reality of chatbots is rarely that simple. \n",
        "Take, for example, the use case for restaurants. People can search for them, ask about the menus, or book tables. These activities require a substantially \n",
        "more advanced configuration.\n",
        "\n",
        "In the purpose of this more realistic demo, we decided to go through a rather unusual route. To simplify the process of defining the domain and behavior \n",
        "of this chatbot, we took a famous industrial research dataset provided by the Dialogue Systems Technology Challenge known as DSTC, also known as [Schema Dataset](https://github.com/google-research-datasets/dstc8-schema-guided-dialogue).\n",
        "\n",
        "This dataset contains a huge number of the annotated human-machine conversations \n",
        "crowdsourced in an [M2M manner](https://arxiv.org/pdf/1801.04871.pdf) for various real-life scenarios and domains.\n",
        "\n",
        "One of these domains is dedicated to *Restaurants*. In it, users are performing a variety of the goal-oriented tasks like searching for restaurants or booking tables via interaction with the bot.\n",
        "\n",
        "Given the power and elegance of the DSTC format, we took liberty to use our internal **automatic conversion tool** to directly **transform** its data into the set of _stories.md_, _nlu.md_, _domain.yml_.\n",
        "\n",
        "*Note: the dataset used is this demo is quite large. The dataset files listings are provided in form of file subset listings. Feel free to examine the files yourself.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XcO1oe7nw7G",
        "colab_type": "text"
      },
      "source": [
        "##### Download the data used in this tutorial section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSgoo2CSk0Ku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "5496eb45-e5da-4bbf-a8a2-d2ea4f39c511"
      },
      "source": [
        "# let's get the mentioned converted Schema-dataset subset\n",
        "!wget http://files.deeppavlov.ai/datasets/schema_resto_md_yaml_v2.tar.gz\n",
        "!tar -zxf schema_resto_md_yaml_v2.tar.gz "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-07 08:22:47--  http://files.deeppavlov.ai/datasets/schema_resto_md_yaml_v2.tar.gz\n",
            "Resolving files.deeppavlov.ai (files.deeppavlov.ai)... 93.175.29.74\n",
            "Connecting to files.deeppavlov.ai (files.deeppavlov.ai)|93.175.29.74|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100665 (98K) [application/octet-stream]\n",
            "Saving to: ‘schema_resto_md_yaml_v2.tar.gz’\n",
            "\n",
            "schema_resto_md_yam 100%[===================>]  98.31K   110KB/s    in 0.9s    \n",
            "\n",
            "2020-08-07 08:22:49 (110 KB/s) - ‘schema_resto_md_yaml_v2.tar.gz’ saved [100665/100665]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-3Jz_IpA9hg",
        "colab_type": "text"
      },
      "source": [
        "#### Technical Note: Automatic Conversion from DSTC Schema Format to RASA DSLs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inqqJqYoBOzX",
        "colab_type": "text"
      },
      "source": [
        "Schema dataset is provided in DSTC (Dialogue State Tracking Challenge) [format](https://github.com/google-research-datasets/dstc8-schema-guided-dialogue).\n",
        "\n",
        "The DSTC format has its own advantages: it is very detailed and allows for various additional info to be incorporated into the dataset itself. \n",
        "\n",
        "In it, there are two major components - Schema Representation, and Dialog Representation. The first component is dedicated to describing sets of Intents, Slots, and Entities that are used by a given service through an API. The second component is focused on describing actual dialogs that happen between users and services. It also includes actual labels for the aforementioned Intents, Slots, and Entities defined in the Schema component. \n",
        "\n",
        "However, while DSTC format is quite rich for building state-of-the-art systems that participate in the annual DSTC competitions, it takes a serious effort for the real-world developers to collect and annotate data using this format. In contrast, RASA DSLs we're illustrating here are quite different from the DSTC: they are meant to be neat and minimalistic, and to allow developers to define their domains from a rather scarce input information. \n",
        "\n",
        "As mentioned in the beginning of this part of the tutorial, we've performed an automatical conversion of the Schema Restaurants dataset from the DSTC format to RASA DSLs.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPvxwuuMC6EY",
        "colab_type": "text"
      },
      "source": [
        "#### Slot Filler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt8LSQtQDDci",
        "colab_type": "text",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Any typical goal-oriented chatbot system uses a standard approach to define the way it works in the form of a pipeline. DeepPavlov's Go-Bot is quite permissive in which components\n",
        "can be used in it's pipeline; however, Slot Filler is the required one.\n",
        "\n",
        "Slot Filler, also known as slotfiller, is necessary to recognize and normalize slot-filling information provided in the user's utterances.\n",
        "\n",
        "For example, when user says that she wants to \"book a table in *London*\", slotfiller's job is to recognize that *London* in this phrase represents the required slot `city`.\n",
        "\n",
        "For the purposes of this demo, we are providing the pretrained slotfiller for the dataset. The small notebook on how the slotfiller was trained will be provided in one of the upcoming releases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab9kyASfzQCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "outputId": "45a10f7d-ce35-468b-d038-d0822021514d"
      },
      "source": [
        "from deeppavlov import configs, train_model, build_model\n",
        "from deeppavlov.core.common.file import read_json\n",
        "\n",
        "!python -m deeppavlov download schema_resto_md_yaml/ner_config.json\n",
        "!python -m deeppavlov download schema_resto_md_yaml/slotfiller_config.json\n",
        "\n",
        "slotfill_config = read_json(\"schema_resto_md_yaml/slotfiller_config.json\")\n",
        "slotfiller = build_model(slotfill_config, download=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:22:54.221 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/datasets/schema_resto_md_yaml_v2.tar.gz?config=ner_config to /root/.deeppavlov/schema_resto_md_yaml_v2.tar.gz\n",
            "100% 101k/101k [00:00<00:00, 170kB/s] \n",
            "2020-08-07 08:22:55.416 INFO in 'deeppavlov.core.data.utils'['utils'] at line 269: Extracting /root/.deeppavlov/schema_resto_md_yaml_v2.tar.gz archive into /root/.deeppavlov/downloads\n",
            "2020-08-07 08:22:56.30 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/deeppavlov_data/schema_ner.tar.gz?config=ner_config to /root/.deeppavlov/schema_ner.tar.gz\n",
            "100% 1.67M/1.67M [00:02<00:00, 808kB/s]\n",
            "2020-08-07 08:22:59.387 INFO in 'deeppavlov.core.data.utils'['utils'] at line 269: Extracting /root/.deeppavlov/schema_ner.tar.gz archive into /root/.deeppavlov/models\n",
            "2020-08-07 08:23:00.715 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt?config=ner_config download because of matching hashes\n",
            "2020-08-07 08:23:04.475 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/deeppavlov_data/schema_ner.tar.gz?config=slotfiller_config download because of matching hashes\n",
            "2020-08-07 08:23:05.923 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/datasets/schema_resto_md_yaml_v2.tar.gz?config=slotfiller_config download because of matching hashes\n",
            "2020-08-07 08:23:07.237 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt?config=slotfiller_config download because of matching hashes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:23:09.511 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/deeppavlov_data/schema_ner.tar.gz download because of matching hashes\n",
            "2020-08-07 08:23:10.822 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt download because of matching hashes\n",
            "2020-08-07 08:23:12.58 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/datasets/schema_resto_md_yaml_v2.tar.gz download because of matching hashes\n",
            "2020-08-07 08:23:12.258 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/schema_ner/word.dict]\n",
            "2020-08-07 08:23:12.261 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/schema_ner/tag.dict]\n",
            "2020-08-07 08:23:12.267 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/schema_ner/char.dict]\n",
            "2020-08-07 08:23:12.269 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `/root/.deeppavlov/downloads/embeddings/glove.6B.100d.txt`]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/ner/network.py:96: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/layers/tf_layers.py:420: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/ner/network.py:211: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/common/check_gpu.py:29: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:23:54.61 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 760: \n",
            "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnLSTMCell later. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/layers/tf_layers.py:733: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/layers/tf_layers.py:865: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "seq_dim is deprecated, use seq_axis instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "batch_dim is deprecated, use batch_axis instead\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:23:54.163 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 760: \n",
            "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnLSTMCell later. \n",
            "2020-08-07 08:23:56.622 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/schema_ner/model_no_pos]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/schema_ner/model_no_pos\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:23:56.836 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 45: Load path '/root/.deeppavlov/downloads/schema_resto_md_yaml/slotfill.json' differs from save path '/root/.deeppavlov/models/slotfill_dstc2/model' in 'infer' mode for DstcSlotFillingNetwork.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6slj0dKFdXZC",
        "colab_type": "code",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "644e4414-8ecf-4fec-f3a2-ece3da0dd507"
      },
      "source": [
        "slotfiller([\"i'm looking for a thai food somewhere in SFO\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'city': 'SFO', 'cuisine': 'Thai'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG8Zk-ypankD",
        "colab_type": "text",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Seems OK. Let's save our slotfiller config to train and evaluate the restaurants bot, finally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DKCnXjGd6Q0",
        "colab_type": "code",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "json.dump(slotfill_config, open('slotfill_config.json', 'wt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtQcZzXTa8e7",
        "colab_type": "text"
      },
      "source": [
        "#### Known Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QVLkqcXbyjt",
        "colab_type": "text"
      },
      "source": [
        "While slotfilling technology uses the power of the industry-standard Named Entity Recognition (NER) method to recognize key slots in the given phrases, \n",
        "the quality of slot recognition can be substantially increased by combining this process with the data already known to the bot's developer. \n",
        "For example, having a finite list of cities that are supported by a given end-user solution (e.g., several cities in the Greater Seattle Area for local restaurant chain) \n",
        "aids slotfiller in a significant way. Typically, this information is stored in the database, though it may also be provided in the loose files like CSV (comma-separated values).\n",
        "\n",
        "However, in order to focus on the support of the RASA DSLs, we made a conscious decision to omit the support of such data in this demo. An additional demo highligting usage of such data will be provided in one of the consequent releases.\n",
        "\n",
        "Nevertheless our demo goal-oriented bot should still be able to generalize and use the global patterns in the conversations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZQepTn7g650",
        "colab_type": "text"
      },
      "source": [
        "### Stories.md: Advanced Stories Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5qaUcdMmO-e",
        "colab_type": "text",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "`stories.md`\n",
        "\n",
        "*Note: As said above, this file has been auto-generated from the DSTC schema.*\n",
        "\n",
        "Like in the Basic Demo, Stories here define a variety of interactions between user and our bot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH81FW6Wdokg",
        "colab_type": "code",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "8b4d8dd9-5892-458c-d863-6b5a5b367da4"
      },
      "source": [
        "STORIES_FPATH = \"schema_resto_md_yaml/stories.md\"\n",
        "!echo \"stories file size (lines): $(wc -l {STORIES_FPATH})\"\n",
        "\n",
        "!echo -e '\\n\\npart of stories file is listed below\\n'\n",
        "!head -500 {STORIES_FPATH} | tail -30"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stories file size (lines): 4768 schema_resto_md_yaml/stories.md\n",
            "\n",
            "\n",
            "part of stories file is listed below\n",
            "\n",
            "  - utter_INFORM_Cuisine+INFORM_StreetAddress+NOTIFY_SUCCESS\n",
            "* THANK_YOU+GOODBYE{}\n",
            "  - utter_GOODBYE\n",
            "\n",
            "## 25\n",
            "* hi{}\n",
            "  - utter_hi\n",
            "* INFORM_INTENT_Intent{\"intent\": \"FindRestaurants\"}\n",
            "  - utter_REQUEST_City\n",
            "* INFORM_City{\"city\": \"Oakland\"}\n",
            "  - utter_REQUEST_Cuisine\n",
            "* INFORM_Cuisine{\"cuisine\": \"Fish\"}\n",
            "  - utter_OFFER_RestaurantName+OFFER_City+INFORM_COUNT_Count\n",
            "* REQUEST_ALTS{}\n",
            "  - utter_OFFER_RestaurantName+OFFER_City\n",
            "* INFORM_PriceRange+REQUEST_ALTS{\"price_range\": \"moderate\"}\n",
            "  - utter_OFFER_RestaurantName+OFFER_City+INFORM_COUNT_Count\n",
            "* REQUEST_ServesAlcohol+REQUEST_HasLiveMusic{}\n",
            "  - utter_INFORM_HasLiveMusic+INFORM_ServesAlcohol\n",
            "* INFORM_INTENT_Intent+SELECT{\"intent\": \"ReserveRestaurant\"}\n",
            "  - utter_REQUEST_Time\n",
            "* INFORM_PartySize+INFORM_Time{\"party_size\": \"2\", \"time\": \"10:30 in the morning\"}\n",
            "  - utter_CONFIRM_RestaurantName+CONFIRM_City+CONFIRM_Time+CONFIRM_PartySize+CONFIRM_Date\n",
            "* REQUEST_StreetAddress+REQUEST_PhoneNumber+AFFIRM{}\n",
            "  - utter_NOTIFY_FAILURE+REQ_MORE\n",
            "* INFORM_PartySize+INFORM_Time+INFORM_INTENT_Intent{\"party_size\": \"1\", \"time\": \"12:30\", \"intent\": \"ReserveRestaurant\"}\n",
            "  - utter_CONFIRM_RestaurantName+CONFIRM_City+CONFIRM_Time+CONFIRM_PartySize+CONFIRM_Date\n",
            "* AFFIRM{}\n",
            "  - utter_NOTIFY_SUCCESS\n",
            "* THANK_YOU+GOODBYE{}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1XYWTabzPmI",
        "colab_type": "text"
      },
      "source": [
        "### nlu.md: Advanced NLU Training Data Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LMcU7YnmRkn",
        "colab_type": "text",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "`nlu.md`\n",
        "\n",
        "*Note: As said above, this file has been auto-generated from the DSTC schema, and it's quite large. Below you can see only a part of this file. Feel free to examine the entire file.*\n",
        "\n",
        "Like in the Basic Demo, `nlu.md` shows the examples of the user utterances for the supported intent classes.\n",
        "The slotfilling and NER information is provided in the form of the inline mark-up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCgB0VugfWhe",
        "colab_type": "code",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "73764189-0336-4286-87ec-33d02800c405"
      },
      "source": [
        "NLU_FPATH = \"schema_resto_md_yaml/nlu.md\"\n",
        "!echo \"nlu file size (lines): $(wc -l {NLU_FPATH})\"\n",
        "\n",
        "!echo -e '\\n\\npart of nlu file is listed below\\n'\n",
        "!head -50 {NLU_FPATH} | tail -20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nlu file size (lines): 2147 schema_resto_md_yaml/nlu.md\n",
            "\n",
            "\n",
            "part of nlu file is listed below\n",
            "\n",
            "- [Evening 7:15](time) at Mcdonald's.\n",
            "- [6:15 in the evening](time) at Bistro liaison.\n",
            "- Let's set it up for [6:30 pm](time) at Sakoon.\n",
            "- I would like a reservation in [Flames Eatery](restaurant_name) at [19:00](time)\n",
            "- I'm thinking about Rice Bowl, at [morning 11:30](time)\n",
            "\n",
            "## intent:REQUEST_Cuisine+AFFIRM\n",
            "- Sounds great. Do you know if they cook [Taiwanese](cuisine) food?\n",
            "- yeah. what do they serve?\n",
            "\n",
            "## intent:AFFIRM\n",
            "- Yes\n",
            "- That's great.\n",
            "- Sure that sounds good.\n",
            "- That works!\n",
            "- that would be great.\n",
            "- That sounds good.\n",
            "- Yes that works for me\n",
            "- Sure, that sounds great!\n",
            "- This is good for me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eNUnEb5bFjB",
        "colab_type": "text"
      },
      "source": [
        "Let's take a closer look to some specific intent examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzyrOJSsICzU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "40eec6d0-267e-4a51-9c9a-dbabf6fe7823"
      },
      "source": [
        "!grep --no-group-separator -m 10 -A 1 -P \"(INFORM_Cuisine|INFORM_City)\" {NLU_FPATH}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "## intent:INFORM_City+INFORM_INTENT_Intent\n",
            "- Can you find me somewhere to eat in Dublin?\n",
            "## intent:INFORM_Cuisine\n",
            "- Anyone that is [burger](cuisine) like in nature\n",
            "## intent:INFORM_City+INFORM_Cuisine\n",
            "- I'm looking for [European](cuisine) food in San Francisco.\n",
            "## intent:INFORM_Cuisine+INFORM_INTENT_Intent\n",
            "- I am looking for a [Greek](cuisine) place to eat please.\n",
            "## intent:INFORM_City\n",
            "- It's in San Francisco.\n",
            "## intent:INFORM_Cuisine+REQUEST_ALTS\n",
            "- I'd like another kind of restaurant. I'd like Sushi.\n",
            "## intent:INFORM_City+INFORM_Time\n",
            "- I want a restaurant in Mountain View, and a reservation at 1 o\"clock in the afternoon.\n",
            "## intent:INFORM_PriceRange+INFORM_Cuisine\n",
            "- I would prefer some [Filipino](cuisine) food which is affordable and not too expensive.\n",
            "## intent:INFORM_City+INFORM_Time+NEGATE\n",
            "- No. Please make it for [7:30 pm](time) at Santa Clara.\n",
            "## intent:INFORM_RestaurantName+INFORM_City+INFORM_Time\n",
            "- the restaurant is la traviata.i want it in san fran.make it at [6:30 pm](time)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "-mXUmoK-zPmQ",
        "colab_type": "text"
      },
      "source": [
        "### domain.yml: Advanced Domain Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIoT26lqmUAa",
        "colab_type": "text"
      },
      "source": [
        "`domain.yml`\n",
        "\n",
        "*Note: As said above, this file has been auto-generated from the DSTC schema, and it's quite large. Below you can see only a part of this file. Feel free to examine the entire file.*\n",
        "\n",
        "The domain file now provides the list of slots and entities as defined by the DSTC schema, as well as the supported intent classes and system response text templates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y14jLZzJfYye",
        "colab_type": "code",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e7ec0e6-2068-4bcb-e58b-79c9ecb2845b"
      },
      "source": [
        "DOMAIN_FPATH = \"schema_resto_md_yaml/domain.yml\"\n",
        "!echo \"domain file size (lines): $(wc -l {DOMAIN_FPATH})\"\n",
        "\n",
        "!echo -e '\\n\\nmost of domain file is listed below, just some portion of intents and response templates is skipped \\n'\n",
        "!head -40 {DOMAIN_FPATH} && echo \"...\"\n",
        "!grep -B 1 -A 10 responses {DOMAIN_FPATH} && echo \"...\"\n",
        "!grep --no-group-separator -A 1 OFFER_City: {DOMAIN_FPATH} && echo \"...\"\n",
        "!grep --no-group-separator -A 1 CONFIRM_Time: {DOMAIN_FPATH} && echo \"...\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "domain file size (lines): 478 schema_resto_md_yaml/domain.yml\n",
            "\n",
            "\n",
            "most of domain file is listed below, just some portion of intents and response templates is skipped \n",
            "\n",
            "slots:\n",
            "  price_range:\n",
            "    type: text\n",
            "  cuisine:\n",
            "    type: text\n",
            "  date:\n",
            "    type: text\n",
            "  restaurant_name:\n",
            "    type: text\n",
            "  intent:\n",
            "    type: text\n",
            "  party_size:\n",
            "    type: text\n",
            "  time:\n",
            "    type: text\n",
            "  has_live_music:\n",
            "    type: text\n",
            "  city:\n",
            "    type: text\n",
            "  serves_alcohol:\n",
            "    type: text\n",
            "\n",
            "entities:\n",
            "- price_range\n",
            "- cuisine\n",
            "- date\n",
            "- restaurant_name\n",
            "- intent\n",
            "- party_size\n",
            "- time\n",
            "- has_live_music\n",
            "- city\n",
            "- serves_alcohol\n",
            "\n",
            "intents:\n",
            "  - INFORM_RestaurantName+INFORM_Time\n",
            "  - THANK_YOU+GOODBYE\n",
            "  - INFORM_Time+INFORM_RestaurantName\n",
            "  - INFORM_PriceRange+REQUEST_ALTS\n",
            "  - REQUEST_PhoneNumber+REQUEST_StreetAddress+AFFIRM\n",
            "...\n",
            "\n",
            "responses:\n",
            "  utter_REQUEST_Cuisine:\n",
            "    - text: \"What are you in the mood for? {cuisine}? {cuisine}\"\n",
            "  utter_REQUEST_City:\n",
            "    - text: \"What city are you interested in?\"\n",
            "  utter_OFFER_RestaurantName+OFFER_City:\n",
            "    - text: \"Great! {restaurant_name} is a great restaurant in {city}\"\n",
            "  utter_INFORM_PhoneNumber+INFORM_HasLiveMusic:\n",
            "    - text: \"The contact number is {phone_number}\"\n",
            "  utter_OFFER_INTENT_Intent:\n",
            "    - text: \"Would you like to reserve a table here?\"\n",
            "...\n",
            "  utter_OFFER_RestaurantName+OFFER_City:\n",
            "    - text: \"Great! {restaurant_name} is a great restaurant in {city}\"\n",
            "...\n",
            "  utter_CONFIRM_RestaurantName+CONFIRM_Time:\n",
            "    - text: \"Ok can u confirm your booking at 7 pm at {restaurant_name}{time}\"\n",
            "  utter_CONFIRM_City+CONFIRM_Time:\n",
            "    - text: \"Please confirm this: The city is {city}. The reservation is at {time}\"\n",
            "...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgXOUIWQmYUO",
        "colab_type": "text",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now that we have all three key files, like in the Basic Demo, we can now proceed with our bot's training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CkliRfVdlxm",
        "colab_type": "code",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "from deeppavlov import configs\n",
        "from deeppavlov.core.common.file import read_json\n",
        "\n",
        "\n",
        "gobot_config = read_json(configs.go_bot.gobot_md_yaml_minimal)\n",
        "gobot_config['chainer']['pipe'][-1]['slot_filler'] = {\"config_path\": \"slotfill_config.json\"}\n",
        "\n",
        "gobot_config['metadata']['variables']['DATA_PATH'] = 'schema_resto_md_yaml'\n",
        "gobot_config['metadata']['variables']['MODEL_PATH'] = '.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuGCARujbbBs",
        "colab_type": "text",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Since our data is the tutorial data we will use the same subsamples for all of train (training set), test (test set), and valid (validation set) subsamples.\n",
        "\n",
        "However, for a real DeepPavlov-based goal-oriented bot you should use different train, test, and valid sample stories.md files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4HWEXlEeW92",
        "colab_type": "code",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "!cp schema_resto_md_yaml/stories.md schema_resto_md_yaml/stories-trn.md\n",
        "!cp schema_resto_md_yaml/stories.md schema_resto_md_yaml/stories-tst.md\n",
        "!cp schema_resto_md_yaml/stories.md schema_resto_md_yaml/stories-val.md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMbazF8ad6_N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "f63d8140-b215-4426-d4ff-b4555137d5a8"
      },
      "source": [
        "from deeppavlov import train_model\n",
        "\n",
        "train_model(gobot_config, download=False);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:35:16.605 INFO in 'deeppavlov.models.go_bot.policy.policy_network'['policy_network'] at line 86: INSIDE PolicyNetwork init(). Initializing PolicyNetwork from checkpoint.\n",
            "2020-08-07 08:35:16.611 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/dp_big_demo_dir/model/policy]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/dp_big_demo_dir/model/policy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk6inTKkeOql",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2280fc16-c679-4f2c-98c7-e3338ba6218a"
      },
      "source": [
        "bot = build_model(gobot_config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:35:16.789 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/dp_big_demo_dir/word.dict]\n",
            "2020-08-07 08:35:17.434 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpu_18leqx]\n",
            "2020-08-07 08:35:18.107 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmpu2tzz5gn]\n",
            "2020-08-07 08:35:18.689 INFO in 'deeppavlov.dataset_readers.dstc2_reader'['dstc2_reader'] at line 112: [loading dialogs from /tmp/tmp4ceyk7ty]\n",
            "2020-08-07 08:35:19.236 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/schema_ner/word.dict]\n",
            "2020-08-07 08:35:19.240 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/schema_ner/tag.dict]\n",
            "2020-08-07 08:35:19.243 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/schema_ner/char.dict]\n",
            "2020-08-07 08:35:19.245 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `/root/.deeppavlov/downloads/embeddings/glove.6B.100d.txt`]\n",
            "2020-08-07 08:36:00.641 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 760: \n",
            "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnLSTMCell later. \n",
            "2020-08-07 08:36:00.732 INFO in 'deeppavlov.core.layers.tf_layers'['tf_layers'] at line 760: \n",
            "Warning! tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell is used. It is okay for inference mode, but if you train your model with this cell it could NOT be used with tf.contrib.cudnn_rnn.CudnnLSTMCell later. \n",
            "2020-08-07 08:36:02.396 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/schema_ner/model_no_pos]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/schema_ner/model_no_pos\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 08:36:02.525 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 45: Load path '/root/.deeppavlov/downloads/schema_resto_md_yaml/slotfill.json' differs from save path '/root/.deeppavlov/models/slotfill_dstc2/model' in 'infer' mode for DstcSlotFillingNetwork.\n",
            "2020-08-07 08:36:02.528 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `/root/.deeppavlov/downloads/embeddings/glove.6B.100d.txt`]\n",
            "2020-08-07 08:36:44.406 INFO in 'deeppavlov.models.go_bot.policy.policy_network'['policy_network'] at line 86: INSIDE PolicyNetwork init(). Initializing PolicyNetwork from checkpoint.\n",
            "2020-08-07 08:36:44.411 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/dp_big_demo_dir/model/policy]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/dp_big_demo_dir/model/policy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX1meeXJxvC1",
        "colab_type": "text"
      },
      "source": [
        "Let's see whether the bot works at all:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dMz9HyRxrz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bot.reset()\n",
        "\n",
        "bot([\"Hey!\"])[0][0].actions_tuple\n",
        "\n",
        "bot.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT31DCObGpA_",
        "colab_type": "text"
      },
      "source": [
        "Ok, let's have a conversation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2Iv0GNgfjlM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0653a4e6-ca6f-4eea-a4e1-b9300f17cc7b"
      },
      "source": [
        "bot.reset()\n",
        "\n",
        "bot([\"Hi!\"])[0][0].actions_tuple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('utter_hi',)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKwg9N2zbD0B",
        "colab_type": "text"
      },
      "source": [
        "Awesome. Seems like our bot performs well. Let's get some action!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c-oNomgswak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "159c5b84-d0c7-426e-931b-fec719027a0d"
      },
      "source": [
        "bot([\"I'd like to find a restaurant for this evening\"])[0][0].actions_tuple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('utter_REQUEST_Cuisine',)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxJwUqtCbJ6N",
        "colab_type": "text"
      },
      "source": [
        "The bot replies with the request to provide the necessary information, and we give it back:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otpYIs_khwPw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab88533e-79a9-4d6b-e8d9-b001671fcd21"
      },
      "source": [
        "bot([\"Somewhere in Oakland, sushi and for two people please\"])[0][0].actions_tuple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('utter_OFFER_RestaurantName', 'OFFER_City', 'INFORM_COUNT_Count')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4Ab-S4WbUJL",
        "colab_type": "text"
      },
      "source": [
        "And so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51ViTAFjtZTq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7be555eb-6ed5-4a77-fa44-7167b55eb5ff"
      },
      "source": [
        "bot([\"Cool! That's what I was looking for, thanks!\"])[0][0].actions_tuple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('utter_REQUEST_Time',)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIgU4aSSbmlS",
        "colab_type": "text"
      },
      "source": [
        "Let's say goodbye to our bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNrBveyzhw7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d5f81a83-709b-49dc-fbb0-ec3ef41d0b25"
      },
      "source": [
        "bot([\"Bye bot\"])[0][0].actions_tuple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('utter_CONFIRM_RestaurantName',\n",
              " 'CONFIRM_City',\n",
              " 'CONFIRM_Time',\n",
              " 'CONFIRM_Date',\n",
              " 'CONFIRM_PartySize')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B--vLcEBbpRp",
        "colab_type": "text"
      },
      "source": [
        "While it'd be nice for it to reply \"Good bye!\", it didn't. Why?\n",
        "\n",
        "Given that the DSTC dataset doesn't support this utterance, our bot can't properly react to such user's response. So to make our bot a bit more polite we have to add the -bye -bye utterances to the training data.\n",
        "\n",
        "Notice that you will have to add it to all 3 files (note that in case of `domain.yml` you have to add one line to *intents* and another two to *responses* sections of the file):\n",
        "\n",
        "#### stories.md\n",
        "```\n",
        "...\n",
        "## goodbye\n",
        "* bye\n",
        "  - utter_bye\n",
        "...\n",
        "```\n",
        "\n",
        "#### nlu.md\n",
        "```\n",
        "...\n",
        "## intent:bye\n",
        "- goodbye\n",
        "- goodnight\n",
        "...\n",
        "```\n",
        "\n",
        "#### domain.yml\n",
        "```\n",
        "...\n",
        "intents:\n",
        "  - bye\n",
        "...\n",
        "responses:\n",
        "  utter_bye:\n",
        "  - text: Bye!\n",
        "...\n",
        "```\n",
        "\n",
        "You will also have to re-use our stories.md for train, test, and valid stories.md files. Again, for the purposes of this demo, we use the same files for stories.md.\n",
        "\n",
        "However, for a real DeepPavlov-based goal-oriented bot you should use different train, test, and valid sample stories.md files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-d8Um4W-TME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sed -i -e \"s|^\\s*$|\\* bye\\n  - utter_BYE\\n|g\" {STORIES_FPATH}  # add bye to each story\n",
        "!echo -e \"  utter_BYE:\\n    - text: \\\"Bye!\\\"\" >> {DOMAIN_FPATH}  # add bye to nlu example\n",
        "!grep -m 1 -A 3 bye ../dp_minimal_demo_dir/nlu.md >> {NLU_FPATH} && echo \"\" >> {NLU_FPATH}  # add bye to response templates\n",
        "!sed -ie \"s|intents:|intents:\\n  - bye|g\" {DOMAIN_FPATH}  # add bye to domain intents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gXmPYK5zPnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp schema_resto_md_yaml/stories.md schema_resto_md_yaml/stories-trn.md\n",
        "!cp schema_resto_md_yaml/stories.md schema_resto_md_yaml/stories-tst.md\n",
        "!cp schema_resto_md_yaml/stories.md schema_resto_md_yaml/stories-val.md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqi0Zs5ozPnr",
        "colab_type": "text"
      },
      "source": [
        "Re-training our bot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk7FADbsKtRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf model  # remove the previous trained model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GNyCYo9zPnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from deeppavlov import train_model\n",
        "\n",
        "train_model(gobot_config, download=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4-55DMsLINe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bot = build_model(gobot_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUW3JcemzPn2",
        "colab_type": "text"
      },
      "source": [
        "Checking it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4ASqT_Fu4TH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c2c7ab77-a9cf-44a4-cfcf-39d1a6ecbc5b"
      },
      "source": [
        "bot.reset()\n",
        "bot([\"Hi!\"])[0][0].actions_tuple\n",
        "bot([\"I'd like to find a restaurant for this evening\"])[0][0].actions_tuple\n",
        "bot([\"Somewhere in Oakland, sushi and for two people please\"])[0][0].actions_tuple\n",
        "bot([\"Cool! That's what I was looking for, thanks!\"])[0][0].actions_tuple\n",
        "bot([\"Bye bot\"])[0][0].actions_tuple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('utter_BYE',)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCUBa5AcOuoG",
        "colab_type": "text"
      },
      "source": [
        "## Comparing with RASA\n",
        "\n",
        "Now that we've run through a couple of demos, let's make sure that our configs work the same way as they do using RASA framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFmipe9GO-hv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fa4e95ba-8c6c-4de5-e0cd-748e174de304"
      },
      "source": [
        "%cd /content\n",
        "!mkdir rasa_demo\n",
        "%cd rasa_demo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/rasa_demo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIkKfZPqPPyp",
        "colab_type": "text"
      },
      "source": [
        "Let's install the RASA library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPTKTHJDPPKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "44ecd8d0-2767-45b5-9432-741f995a384f"
      },
      "source": [
        "!pip install rasa==1.10.10\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy link en_core_web_md en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rasa\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/5c/3a87ff8595a9b0d1cde2cd19f857a9959c532e8681eefceab49e6ab6f637/rasa-1.10.10-py3-none-any.whl (511kB)\n",
            "\r\u001b[K     |▋                               | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███▏                            | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 512kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16 in /usr/local/lib/python3.6/dist-packages (from rasa) (1.18.0)\n",
            "Requirement already satisfied: attrs<19.4,>=19.3 in /usr/local/lib/python3.6/dist-packages (from rasa) (19.3.0)\n",
            "Collecting rasa-sdk<2.0.0,>=1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/79/fd/af7ad469205c2649eb30cc304d56211d165cd582ab815fb565cc1a7e1ba2/rasa_sdk-1.10.2-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client==4.1.3 in /usr/local/lib/python3.6/dist-packages (from rasa) (4.1.3)\n",
            "Collecting kafka-python<2.0,>=1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/c9/9863483a1353700ba87821b4f39085eb18fd1bcbb1e954c697177d67f03f/kafka_python-1.4.7-py2.py3-none-any.whl (266kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 11.8MB/s \n",
            "\u001b[?25hCollecting pydot<1.5,>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
            "Collecting python-engineio<3.13,>=3.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/aa/c975982df73c4bcd087732db14b05306e8a3f3f24596cc18647746539290/python_engineio-3.12.1-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3<2.0,>=1.12 in /usr/local/lib/python3.6/dist-packages (from rasa) (1.14.33)\n",
            "Requirement already satisfied: cloudpickle<1.4,>=1.2 in /usr/local/lib/python3.6/dist-packages (from rasa) (1.3.0)\n",
            "Collecting jsonpickle<1.5,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
            "Collecting mattermostwrapper<2.3,>=2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/fd/f1ce046ddaeffa5073f87d7800c27ad2c8e543e924a8418675c64aea6a14/mattermostwrapper-2.2.tar.gz\n",
            "Collecting coloredlogs<11.0,>=10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/0f/7877fc42fff0b9d70b6442df62d53b3868d3a6ad1b876bdb54335b30ff23/coloredlogs-10.0-py2.py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hCollecting redis<4.0,>=3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from rasa) (2.4)\n",
            "Collecting python-socketio<4.6,>=4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/cb/631c0b713daea3938e66d4c0923e88f3c0b57b026f860ea76e0337bc9c7a/python_socketio-4.5.1-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hCollecting tensorflow<2.2,>=2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/22/b81f319687fc59bf25e52e1f0959c88c93d55a4fd73eedd2e89e61499208/tensorflow-2.1.1-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 30kB/s \n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy<1.4.0,>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from rasa) (1.3.18)\n",
            "Collecting python-telegram-bot<13.0,>=11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2d/c72fc9a28144277f6170f2fcbfd3bd9427943497522b2689846596eb86cf/python_telegram_bot-12.8-py2.py3-none-any.whl (375kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py<0.10,>=0.9 in /usr/local/lib/python3.6/dist-packages (from rasa) (0.9.0)\n",
            "Collecting tensorflow-addons<0.8.0,>=0.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/1a/988536d55ff814069d851731f31665b0a30989c496d64e0ff3a5efa42f63/tensorflow_addons-0.7.1-cp36-cp36m-manylinux2010_x86_64.whl (986kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 43.0MB/s \n",
            "\u001b[?25hCollecting colorclass<2.3,>=2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/37/ea/ae8dbb956939d4392e6a7fdef87fda273854da1128edae016c4104240be8/colorclass-2.2.0.tar.gz\n",
            "Collecting ujson<3.0,>=1.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/e4/a79c57e22d6d09bbeb5e8febb8cfa0fe10ede69eed9c3458d3ec99014e20/ujson-2.0.3-cp36-cp36m-manylinux1_x86_64.whl (174kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 36.0MB/s \n",
            "\u001b[?25hCollecting aiohttp<3.7,>=3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/39/7eb5f98d24904e0f6d3edb505d4aa60e3ef83c0a58d6fe18244a51757247/aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow_hub<0.9,>=0.7 in /usr/local/lib/python3.6/dist-packages (from rasa) (0.8.0)\n",
            "Collecting terminaltables<3.2.0,>=3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting sanic<20.0.0,>=19.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/54/17f1e496599214dede67e37e019ce2f210b7861d2dd39b92ac4d3d08e83a/sanic-19.12.2-py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.6MB/s \n",
            "\u001b[?25hCollecting sanic-jwt<1.5.0,>=1.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/ca/61b5c31074890ae62cfd00edc940450a9575d46371a92974301595b18edf/sanic-jwt-1.4.1.tar.gz\n",
            "Collecting regex<2020.7,>=2020.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/a1/6d8fdf4a20ffbbf2bd6003dff47a0628b9e6a4b840c421b0dec27da9376e/regex-2020.6.8-cp36-cp36m-manylinux2010_x86_64.whl (660kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 36.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from rasa) (1.4.1)\n",
            "Collecting prompt-toolkit<3.0,>=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/61/2dfea88583d5454e3a64f9308a686071d58d59a55db638268a6413e1eb6d/prompt_toolkit-2.0.10-py3-none-any.whl (340kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 42.8MB/s \n",
            "\u001b[?25hCollecting requests<3.0,>=2.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25hCollecting fbmessenger<6.1.0,>=6.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/e9/646684226176782b9e3b7dd5b35d7ecfd1d13cba24ad2e33255079921aab/fbmessenger-6.0.0-py2.py3-none-any.whl\n",
            "Collecting PyJWT<1.8,>=1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
            "Collecting psycopg2-binary<2.9.0,>=2.8.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/8a/a7ed55c2c55bd4f5844d72734fedc0cef8a74518a0a19105a21c15628f1e/psycopg2_binary-2.8.5-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.0MB/s \n",
            "\u001b[?25hCollecting sanic-cors<0.11.0,>=0.10.0b1\n",
            "  Downloading https://files.pythonhosted.org/packages/2d/33/5e1776669aa62dd9c65e3513425077915acb1758d6b19f08f830f27ce9a8/Sanic_Cors-0.10.0.post3-py2.py3-none-any.whl\n",
            "Collecting tensorflow-estimator==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 41.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: multidict<5.0,>=4.6 in /usr/local/lib/python3.6/dist-packages (from rasa) (4.7.6)\n",
            "Requirement already satisfied: packaging<21.0,>=20.0 in /usr/local/lib/python3.6/dist-packages (from rasa) (20.4)\n",
            "Requirement already satisfied: matplotlib<3.3,>=3.1 in /usr/local/lib/python3.6/dist-packages (from rasa) (3.2.2)\n",
            "Collecting async_generator<1.11,>=1.10\n",
            "  Downloading https://files.pythonhosted.org/packages/71/52/39d20e03abd0ac9159c162ec24b93fbcaa111e8400308f2465432495ca2b/async_generator-1.10-py3-none-any.whl\n",
            "Collecting sklearn-crfsuite<0.4,>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Collecting gevent<1.6,>=1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/95/b53b78b15abbe547bed7381ca9c8319c86d6b646a30d0831e26c307a5fa7/gevent-1.5.0-cp36-cp36m-manylinux2010_x86_64.whl (5.1MB)\n",
            "\u001b[K     |████████████████████████████████| 5.1MB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<2.9,>=2.8 in /usr/local/lib/python3.6/dist-packages (from rasa) (2.8.1)\n",
            "Collecting tensorflow-probability<0.10,>=0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/ed/f587d64127bbb85e539f06a2aace1240b7b5c6b4267bea94f232230551a5/tensorflow_probability-0.9.0-py2.py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 32.5MB/s \n",
            "\u001b[?25hCollecting pika<1.2.0,>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/ae/8bedf0e9f1c0c5d046db3a7428a4227fe36ec1b8e25607f3c38ac9bf513c/pika-1.1.0-py2.py3-none-any.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 45.8MB/s \n",
            "\u001b[?25hCollecting pykwalify<1.8.0,>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/9f/612de8ca540bd24d604f544248c4c46e9db76f6ea5eb75fb4244da6ebbf0/pykwalify-1.7.0-py2.py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 5.3MB/s \n",
            "\u001b[?25hCollecting colorhash<1.1.0,>=1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/0e/e1/50dbc513aa74e99eca4c47f2a8206711f0bec436fdddd95eebaf7eaaa1aa/colorhash-1.0.2-py2.py3-none-any.whl\n",
            "Collecting pymongo[srv,tls]<3.9.0,>=3.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/4a/586826433281ca285f0201235fccf63cc29a30fa78bcd72b6a34e365972d/pymongo-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (416kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 39.4MB/s \n",
            "\u001b[?25hCollecting twilio<6.27,>=6.26\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/e6/630676e9749be27879957dcac080dbafa2a8bf2cf47db3f7247862dd6277/twilio-6.26.3-py2.py3-none-any.whl (979kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from rasa) (49.2.0)\n",
            "Collecting ruamel.yaml<0.17,>=0.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/92/59af3e38227b9cc14520bf1e59516d99ceca53e3b8448094248171e9432b/ruamel.yaml-0.16.10-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 47.0MB/s \n",
            "\u001b[?25hCollecting apscheduler<3.7,>=3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/34/9ef20ed473c4fd2c3df54ef77a27ae3fc7500b16b192add4720cab8b2c09/APScheduler-3.6.3-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hCollecting webexteamssdk<1.4.0,>=1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/99/0e5d4ed08d14853ddf9e076beee5f8e524d22dc0fca06ef08bf952c683d7/webexteamssdk-1.3.tar.gz (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
            "\u001b[?25hCollecting questionary<1.6.0,>=1.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/05/7d/61b7d0da15bb50e7239c870771320026447b7e2d9490ee96f49dddd3ef0d/questionary-1.5.2-py3-none-any.whl\n",
            "Collecting rocketchat_API<1.4.0,>=0.6.31\n",
            "  Downloading https://files.pythonhosted.org/packages/42/12/055da56d3eb5012bb02c806ad4f3549d910147a10dee7ce4b89ccf23f388/rocketchat_API-1.3.1-py3-none-any.whl\n",
            "Requirement already satisfied: pytz<2020.0,>=2019.1 in /usr/local/lib/python3.6/dist-packages (from rasa) (2019.1)\n",
            "Collecting jsonschema<3.3,>=3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.46,>=4.31 in /usr/local/lib/python3.6/dist-packages (from rasa) (4.41.1)\n",
            "Collecting scikit-learn<0.23,>=0.22\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/d8/312e03adf4c78663e17d802fe2440072376fee46cada1404f1727ed77a32/scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 41.4MB/s \n",
            "\u001b[?25hCollecting slackclient<3.0.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/20/8ad76cdab60a3918fb4be96fe5f936f10a8fb03690e7a8cd8d9d0998a89a/slackclient-2.8.0-py2.py3-none-any.whl (92kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client==4.1.3->rasa) (1.15.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client==4.1.3->rasa) (0.17.4)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client==4.1.3->rasa) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client==4.1.3->rasa) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client==4.1.3->rasa) (0.2.8)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot<1.5,>=1.4->rasa) (2.4.7)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3<2.0,>=1.12->rasa) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3<2.0,>=1.12->rasa) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3<2.0,>=1.12->rasa) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle<1.5,>=1.3->rasa) (1.7.0)\n",
            "Collecting humanfriendly>=4.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/2d/2f1b0a780b8c948c06c74c8c80e68ac354da52397ba432a1c5ac1923c3af/humanfriendly-8.2-py2.py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx<2.5.0,>=2.4.0->rasa) (4.4.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1->rasa) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1->rasa) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1->rasa) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1->rasa) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1->rasa) (3.3.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1->rasa) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1->rasa) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1->rasa) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1->rasa) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1->rasa) (1.30.0)\n",
            "Collecting keras-preprocessing==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot<13.0,>=11.1->rasa) (2020.6.20)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot<13.0,>=11.1->rasa) (3.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot<13.0,>=11.1->rasa) (5.1.1)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp<3.7,>=3.6->rasa) (1.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp<3.7,>=3.6->rasa) (3.7.4.2)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp<3.7,>=3.6->rasa) (3.0.4)\n",
            "Requirement already satisfied: httptools>=0.0.10 in /usr/local/lib/python3.6/dist-packages (from sanic<20.0.0,>=19.12.2->rasa) (0.0.13)\n",
            "Collecting aiofiles>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/2b/078a9771ae4b67e36b0c2a973df845260833a4eb088b81c84b738509b4c4/aiofiles-0.5.0-py3-none-any.whl\n",
            "Collecting httpx==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/17/3f1ec0593b38c82069e745c849114267e980c9fb1254a27ab50f72040251/httpx-0.9.3-py2.py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: websockets<9.0,>=7.0 in /usr/local/lib/python3.6/dist-packages (from sanic<20.0.0,>=19.12.2->rasa) (8.1)\n",
            "Requirement already satisfied: uvloop>=0.5.3; sys_platform != \"win32\" and implementation_name == \"cpython\" in /usr/local/lib/python3.6/dist-packages (from sanic<20.0.0,>=19.12.2->rasa) (0.14.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<3.0,>=2.0->rasa) (0.2.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.23->rasa) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.23->rasa) (1.24.3)\n",
            "Collecting sanic-plugins-framework>=0.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/17/33/39c4466ff47e7bc2f43f7491e1134ccdf658e2c5e9d60512b01f79d70f9c/Sanic_Plugins_Framework-0.9.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.3,>=3.1->rasa) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.3,>=3.1->rasa) (0.10.0)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite<0.4,>=0.3->rasa) (0.8.7)\n",
            "Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/a4/0d8685c98986326534b0753a8b92b3082bc9df42b348bc50d6c69839c9f9/greenlet-0.4.16-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from pykwalify<1.8.0,>=1.7.0->rasa) (0.6.2)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from pykwalify<1.8.0,>=1.7.0->rasa) (3.13)\n",
            "Collecting dnspython<2.0.0,>=1.13.0; extra == \"srv\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d3/3aa0e7213ef72b8585747aa0e271a9523e713813b9a20177ebe1e939deb0/dnspython-1.16.0-py2.py3-none-any.whl (188kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pysocks; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from twilio<6.27,>=6.26->rasa) (1.7.1)\n",
            "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/77/4bcd63f362bcb6c8f4f06253c11f9772f64189bf08cf3f40c5ccbda9e561/ruamel.yaml.clib-0.2.0-cp36-cp36m-manylinux1_x86_64.whl (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.6/dist-packages (from apscheduler<3.7,>=3.6->rasa) (1.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from webexteamssdk<1.4.0,>=1.1.1->rasa) (0.16.0)\n",
            "Collecting requests-toolbelt\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema<3.3,>=3.2->rasa) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.23,>=0.22->rasa) (0.16.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3<2.0,>=1.12->rasa) (0.15.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle<1.5,>=1.3->rasa) (3.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow<2.2,>=2.1->rasa) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1->rasa) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1->rasa) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1->rasa) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1->rasa) (1.17.2)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-telegram-bot<13.0,>=11.1->rasa) (1.14.1)\n",
            "Collecting h11==0.8.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/f3/8e4cf5fa1a3d8bda942a0b1cf92f87815494216fd439f82eb99073141ba0/h11-0.8.1-py2.py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
            "\u001b[?25hCollecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/38/c4fa6943e6e8e9d854f887631b99fdb1d209c615584169613c9970c7751d/hstspreload-2020.8.5-py3-none-any.whl (930kB)\n",
            "\u001b[K     |████████████████████████████████| 931kB 41.9MB/s \n",
            "\u001b[?25hCollecting sniffio==1.*\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/82/4bd4b7d9c0d1dc0fbfbc2a1e00138e7f3ab85bc239358fe9b78aa2ab586d/sniffio-1.1.0-py3-none-any.whl\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Collecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1->rasa) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1->rasa) (4.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->python-telegram-bot<13.0,>=11.1->rasa) (2.20)\n",
            "Collecting contextvars>=2.1; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1->rasa) (3.1.0)\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mattermostwrapper, colorclass, terminaltables, sanic-jwt, webexteamssdk, idna-ssl, contextvars\n",
            "  Building wheel for mattermostwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mattermostwrapper: filename=mattermostwrapper-2.2-cp36-none-any.whl size=2466 sha256=6e30798eeb2281e2a6e2f39f2ec6e50c62999e1237ed545a98585e9622e9ae1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/47/19/47188b3036316651250c4f7df23d59a3b524c82921bfb6daa3\n",
            "  Building wheel for colorclass (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colorclass: filename=colorclass-2.2.0-cp36-none-any.whl size=19396 sha256=c939f4d8623be85bf9f84e64c74399d6623cc9ee9d8a8cb1d66a09badea241bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/86/9d/16127127306a92d7fd30267890a5634026c045391979c4c317\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=9880bd957c2270e527eb0576e0f00a77907c5f8cbccc37bacdd5143437bdcf8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "  Building wheel for sanic-jwt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sanic-jwt: filename=sanic_jwt-1.4.1-cp36-none-any.whl size=21614 sha256=78d38d9e254a5fbc634c8bfa7b7dfc4b44425f5dc2a165352ea09056b8a61dc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/f7/9b/a6ff52d9805d0bca73082fb43a2dd46283138d83fd453a62fb\n",
            "  Building wheel for webexteamssdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for webexteamssdk: filename=webexteamssdk-1.3-cp36-none-any.whl size=99584 sha256=008823593091e1a5b69e8717d9ef3977489383f496bbdba481f3ae925a1ad3d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/0c/40/a86821ed5e4ff74c9b031cb4504f2e929752573c452cd2a1d8\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3162 sha256=26a633e415bd221d5ca45a450b029a31a735c18e4fabf681cd3db6b813fe430e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=0bf9af1d17f80ca71efabec19ec8bd41ba3f58dc87297bc26bdfa86eafc7bbc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built mattermostwrapper colorclass terminaltables sanic-jwt webexteamssdk idna-ssl contextvars\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: ipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.11.0 has requirement requests==2.22.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.11.0 has requirement ruamel.yaml==0.15.100, but you'll have ruamel-yaml 0.16.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.11.0 has requirement scikit-learn==0.21.2, but you'll have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: aiofiles, h11, hstspreload, immutables, contextvars, sniffio, rfc3986, hyperframe, hpack, h2, httpx, ujson, sanic, requests, sanic-plugins-framework, sanic-cors, humanfriendly, coloredlogs, rasa-sdk, kafka-python, pydot, python-engineio, jsonpickle, mattermostwrapper, redis, python-socketio, tensorflow-estimator, tensorboard, keras-preprocessing, tensorflow, python-telegram-bot, tensorflow-addons, colorclass, idna-ssl, async-timeout, aiohttp, terminaltables, PyJWT, sanic-jwt, regex, prompt-toolkit, fbmessenger, psycopg2-binary, async-generator, python-crfsuite, sklearn-crfsuite, greenlet, gevent, tensorflow-probability, pika, pykwalify, colorhash, dnspython, pymongo, twilio, ruamel.yaml.clib, ruamel.yaml, apscheduler, requests-toolbelt, webexteamssdk, questionary, rocketchat-API, jsonschema, scikit-learn, slackclient, rasa\n",
            "  Found existing installation: h11 0.9.0\n",
            "    Uninstalling h11-0.9.0:\n",
            "      Successfully uninstalled h11-0.9.0\n",
            "  Found existing installation: requests 2.22.0\n",
            "    Uninstalling requests-2.22.0:\n",
            "      Successfully uninstalled requests-2.22.0\n",
            "  Found existing installation: pydot 1.3.0\n",
            "    Uninstalling pydot-1.3.0:\n",
            "      Successfully uninstalled pydot-1.3.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Found existing installation: tensorflow 1.15.2\n",
            "    Uninstalling tensorflow-1.15.2:\n",
            "      Successfully uninstalled tensorflow-1.15.2\n",
            "  Found existing installation: tensorflow-addons 0.8.3\n",
            "    Uninstalling tensorflow-addons-0.8.3:\n",
            "      Successfully uninstalled tensorflow-addons-0.8.3\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: tensorflow-probability 0.11.0\n",
            "    Uninstalling tensorflow-probability-0.11.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.11.0\n",
            "  Found existing installation: pymongo 3.11.0\n",
            "    Uninstalling pymongo-3.11.0:\n",
            "      Successfully uninstalled pymongo-3.11.0\n",
            "  Found existing installation: ruamel.yaml 0.15.100\n",
            "    Uninstalling ruamel.yaml-0.15.100:\n",
            "      Successfully uninstalled ruamel.yaml-0.15.100\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Found existing installation: scikit-learn 0.21.2\n",
            "    Uninstalling scikit-learn-0.21.2:\n",
            "      Successfully uninstalled scikit-learn-0.21.2\n",
            "Successfully installed PyJWT-1.7.1 aiofiles-0.5.0 aiohttp-3.6.2 apscheduler-3.6.3 async-generator-1.10 async-timeout-3.0.1 colorclass-2.2.0 coloredlogs-10.0 colorhash-1.0.2 contextvars-2.4 dnspython-1.16.0 fbmessenger-6.0.0 gevent-1.5.0 greenlet-0.4.16 h11-0.8.1 h2-3.2.0 hpack-3.0.0 hstspreload-2020.8.5 httpx-0.9.3 humanfriendly-8.2 hyperframe-5.2.0 idna-ssl-1.1.0 immutables-0.14 jsonpickle-1.4.1 jsonschema-3.2.0 kafka-python-1.4.7 keras-preprocessing-1.1.0 mattermostwrapper-2.2 pika-1.1.0 prompt-toolkit-2.0.10 psycopg2-binary-2.8.5 pydot-1.4.1 pykwalify-1.7.0 pymongo-3.8.0 python-crfsuite-0.9.7 python-engineio-3.12.1 python-socketio-4.5.1 python-telegram-bot-12.8 questionary-1.5.2 rasa-1.10.10 rasa-sdk-1.10.2 redis-3.5.3 regex-2020.6.8 requests-2.24.0 requests-toolbelt-0.9.1 rfc3986-1.4.0 rocketchat-API-1.3.1 ruamel.yaml-0.16.10 ruamel.yaml.clib-0.2.0 sanic-19.12.2 sanic-cors-0.10.0.post3 sanic-jwt-1.4.1 sanic-plugins-framework-0.9.3 scikit-learn-0.22.2.post1 sklearn-crfsuite-0.3.6 slackclient-2.8.0 sniffio-1.1.0 tensorboard-2.1.1 tensorflow-2.1.1 tensorflow-addons-0.7.1 tensorflow-estimator-2.1.0 tensorflow-probability-0.9.0 terminaltables-3.1.0 twilio-6.26.3 ujson-2.0.3 webexteamssdk-1.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_ruamel_yaml",
                  "jsonschema",
                  "keras_preprocessing",
                  "prompt_toolkit",
                  "regex",
                  "requests",
                  "ruamel",
                  "sklearn",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qeNKP1YPPAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "!cp ../dp_big_demo_dir/schema_resto_md_yaml/{stories,nlu}.md data\n",
        "!cp ../dp_big_demo_dir/schema_resto_md_yaml/domain.yml ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrLERQsARQ9R",
        "colab_type": "text"
      },
      "source": [
        "We'll also use some simple rasa environment config from their repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeTHKjemRaAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "6b966769-2200-45a3-f446-c6d8afafa7bd"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/RasaHQ/rasa/1.10.x/examples/moodbot/config.yml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-07 09:34:48--  https://raw.githubusercontent.com/RasaHQ/rasa/master/examples/moodbot/config.yml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 290 [text/plain]\n",
            "Saving to: ‘config.yml’\n",
            "\n",
            "config.yml          100%[===================>]     290  --.-KB/s    in 0s      \n",
            "\n",
            "2020-08-07 09:34:49 (9.70 MB/s) - ‘config.yml’ saved [290/290]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mczC9E7POar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d5796a4-f21f-4bd0-d7f6-3eea7ed839c2"
      },
      "source": [
        "!rasa train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[93m/usr/local/lib/python3.6/dist-packages/rasa/core/domain.py:151: FutureWarning: No tracker session configuration was found in the loaded domain. Domains without a session config will automatically receive a session expiration time of 60 minutes in Rasa version 2.0 if not configured otherwise.\n",
            "  session_config = cls._get_session_config(data.get(SESSION_CONFIG_KEY, {}))\n",
            "\u001b[0m\u001b[94mTraining Core model...\u001b[0m\n",
            "2020-08-07 09:34:58 \u001b[1;30mINFO    \u001b[0m \u001b[34mroot\u001b[0m  - Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt\n",
            "2020-08-07 09:34:58 \u001b[1;30mINFO    \u001b[0m \u001b[34mroot\u001b[0m  - Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt\n",
            "\u001b[93m/usr/local/lib/python3.6/dist-packages/rasa/core/domain.py:151: FutureWarning: No tracker session configuration was found in the loaded domain. Domains without a session config will automatically receive a session expiration time of 60 minutes in Rasa version 2.0 if not configured otherwise.\n",
            "  session_config = cls._get_session_config(data.get(SESSION_CONFIG_KEY, {}))\n",
            "Processed Story Blocks: 100% 234/234 [00:00<00:00, 988.33it/s, # trackers=1]\n",
            "Processed Story Blocks: 100% 234/234 [00:10<00:00, 21.75it/s, # trackers=37]\n",
            "Processed Story Blocks: 100% 234/234 [00:13<00:00, 17.67it/s, # trackers=46]\n",
            "Processed Story Blocks: 100% 234/234 [00:12<00:00, 18.04it/s, # trackers=42]\n",
            "Processed trackers: 100% 734/734 [00:13<00:00, 52.82it/s, # actions=11213]\n",
            "2020-08-07 09:35:53 \u001b[1;30mWARNING \u001b[0m \u001b[34mrasa.core.featurizers\u001b[0m  - \u001b[33mFeature 'intent_bye' could not be found in feature map.\u001b[0m\n",
            "2020-08-07 09:35:55.744063: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Epochs:  24% 24/100 [27:55<1:06:19, 52.36s/it, t_loss=1.275, loss=0.917, acc=0.971]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NULeLIrNSRJx",
        "colab_type": "text"
      },
      "source": [
        "And when the bot is trained you can interact with it in the interactive mode:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXrnXwmLTsb0",
        "colab_type": "code",
        "colab": {},
        "outputId": "e8040381-de2d-4d0e-c47b-ad07a3a03eb4"
      },
      "source": [
        "!cat | rasa shell"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 12:54:29 \u001b[1;30mINFO    \u001b[0m \u001b[34mroot\u001b[0m  - Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt\n",
            "2020-08-07 12:54:29 \u001b[1;30mINFO    \u001b[0m \u001b[34mroot\u001b[0m  - Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt\n",
            "2020-08-07 12:54:29 \u001b[1;30mINFO    \u001b[0m \u001b[34mroot\u001b[0m  - Starting Rasa server on http://localhost:5005\n",
            "2020-08-07 12:54:45 \u001b[1;30mINFO    \u001b[0m \u001b[34mrasa.nlu.components\u001b[0m  - Added 'SpacyNLP' to component cache. Key 'SpacyNLP-en'.\n",
            "2020-08-07 12:54:51 \u001b[1;30mINFO    \u001b[0m \u001b[34mroot\u001b[0m  - Rasa server is up and running.\n",
            "\u001b[92mBot loaded. Type a message and press enter (use '/stop' to exit): \u001b[0m\n",
            "Warning: Input is not to a terminal (fd=0).\n",
            "\u001b[15C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25hHi!\n",
            "\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[94mhi\u001b[0m\n",
            "\u001b[15C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25hI'd like to find a restaurant for this evening\n",
            "\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[94mDo you have a preferred city and place?\u001b[0m\n",
            "\u001b[15C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25hSomewhere in Oakland, sushi and for two people please\n",
            "\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[94mDo you have a restaurant you prefer? Can you tell me the time of the reservation?\u001b[0m\n",
            "\u001b[15C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25hCool! That's what I was looking for, thanks!\n",
            "\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[94mHave a good one.\u001b[0m\n",
            "\u001b[15C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25hBye bot\n",
            "\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[94mhi\u001b[0m\n",
            "\u001b[15C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25hBye bot\n",
            "\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[94mhi\u001b[0m\n",
            "\u001b[15C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25hbye\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Excellent. While this more Advanced demo uses the auto-generated set of stories, intents, slots, and entities, these files were successfully processed by RASA's framework, and RASA-based trained bot supports the same dialog flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug6-3fYwb-BX",
        "colab_type": "text"
      },
      "source": [
        "## Final words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzcVEXleb_aU",
        "colab_type": "text"
      },
      "source": [
        "While DSTC dataset is quite rich and powerful, real-world use cases rarely if ever give bot developers a luxury of such well-annotated detailed dataset for their use cases.\n",
        "\n",
        "By using the power of DSLs like RASA's, developers can significantly cut the time needed to design their bots from scratch.\n",
        "\n",
        "In this tutorial, we showed you two demos that show how you can define the domain logic of your assistant using RASA DSLs and build your own bot using the power of the DeepPavlov Go-Bot ML-driven technology.\n",
        "\n",
        "We encourage you to try it out, and we would love to hear your thoughts on it. \n",
        "\n",
        "More in-depth examples of the ML-driven goal-oriented bots and their features are coming with the future releases, so stay tuned!"
      ]
    }
  ]
}
